{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Melanoma Classification (Kaggle) \n",
        "\n",
        "Colab-ready notebook for training an EfficientNet-based melanoma classifier on the Kaggle Melanoma Skin Cancer dataset. The notebook downloads the dataset via Kaggle API, performs EDA, handles class imbalance, trains with callbacks, and saves the model to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**How to use in Colab**\n",
        "- Upload your `kaggle.json` when prompted (Kaggle > Account > Create API Token).\n",
        "- Runtime: GPU recommended (Runtime > Change runtime type > GPU).\n",
        "- Everything else runs end-to-end in Colab (no local files needed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install lightweight dependencies (TensorFlow is preinstalled on Colab)\n",
        "!pip install -q kaggle seaborn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
        "                             precision_recall_fscore_support)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import files, drive\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download dataset via Kaggle API\n",
        "- Upload `kaggle.json` when prompted below.\n",
        "- The dataset is saved under `/content/data` and automatically unzipped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_DIR = Path(\"/content/data\")\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Configure Kaggle credentials\n",
        "if not Path(\"/root/.kaggle/kaggle.json\").exists():\n",
        "    print(\"\n",
        "Please upload your kaggle.json (Kaggle > Account > Create API Token)\")\n",
        "    uploaded = files.upload()\n",
        "    if \"kaggle.json\" not in uploaded:\n",
        "        raise FileNotFoundError(\"kaggle.json is required to access the Kaggle API\")\n",
        "    os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "    shutil.move(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")\n",
        "    os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
        "else:\n",
        "    print(\"kaggle.json already present; skipping upload\")\n",
        "\n",
        "# Download + unzip (idempotent)\n",
        "!kaggle datasets download -d bhaveshmittal/melanoma-cancer-dataset -p /content/data -q --force\n",
        "!unzip -qo /content/data/melanoma-cancer-dataset.zip -d /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Locate dataset folders and class subdirectories\n",
        "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
        "\n",
        "def has_images(path: Path) -> bool:\n",
        "    try:\n",
        "        return any((f.is_file() and f.suffix.lower() in IMAGE_EXTS) for f in path.iterdir())\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def find_dataset_root(download_root: Path) -> Path:\n",
        "    candidates = sorted([p for p in download_root.glob(\"*\") if p.is_dir()], key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    keyword_hits = [p for p in candidates if \"melanoma\" in p.name.lower() or \"cancer\" in p.name.lower()]\n",
        "    if keyword_hits:\n",
        "        return keyword_hits[0]\n",
        "    return candidates[0] if candidates else download_root\n",
        "\n",
        "def find_split_dir(root: Path, names):\n",
        "    for name in names:\n",
        "        candidate = root / name\n",
        "        if candidate.exists():\n",
        "            return candidate\n",
        "    return None\n",
        "\n",
        "def find_class_dir(base_dir: Path | None, fallback_root: Path) -> Path | None:\n",
        "    search_root = base_dir if base_dir and base_dir.exists() else fallback_root\n",
        "    candidates = []\n",
        "    for path in search_root.rglob(\"*\"):\n",
        "        if path.is_dir():\n",
        "            subdirs = [d for d in path.iterdir() if d.is_dir()]\n",
        "            if len(subdirs) >= 2 and all(has_images(sd) for sd in subdirs):\n",
        "                candidates.append(path)\n",
        "    if candidates:\n",
        "        candidates = sorted(candidates, key=lambda p: sum(1 for _ in p.rglob(\"*\")), reverse=True)\n",
        "        return candidates[0]\n",
        "    return None\n",
        "\n",
        "dataset_root = find_dataset_root(DATA_DIR)\n",
        "train_dir = find_split_dir(dataset_root, [\"train\", \"training\"])\n",
        "val_dir = find_split_dir(dataset_root, [\"val\", \"valid\", \"validation\"])\n",
        "test_dir = find_split_dir(dataset_root, [\"test\", \"testing\"])\n",
        "\n",
        "train_dir = find_class_dir(train_dir, dataset_root)\n",
        "val_dir = find_class_dir(val_dir, dataset_root) if val_dir else None\n",
        "test_dir = find_class_dir(test_dir, dataset_root) if test_dir else None\n",
        "\n",
        "print(f\"Dataset root: {dataset_root}\")\n",
        "print(f\"Train dir:  {train_dir}\")\n",
        "print(f\"Val dir:    {val_dir}\")\n",
        "print(f\"Test dir:   {test_dir}\")\n",
        "assert train_dir is not None, \"Could not locate train directory with class subfolders."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Gather file paths and labels\n",
        "\n",
        "def gather_files(root: Path):\n",
        "    files, labels = [], []\n",
        "    for class_dir in sorted([d for d in root.iterdir() if d.is_dir()]):\n",
        "        class_files = [p for p in class_dir.rglob(\"*\") if p.is_file() and p.suffix.lower() in IMAGE_EXTS]\n",
        "        if not class_files:\n",
        "            continue\n",
        "        files.extend(class_files)\n",
        "        labels.extend([class_dir.name] * len(class_files))\n",
        "    return files, labels\n",
        "\n",
        "train_files, train_labels = gather_files(train_dir)\n",
        "val_files, val_labels = gather_files(val_dir) if val_dir else ([], [])\n",
        "test_files, test_labels = gather_files(test_dir) if test_dir else ([], [])\n",
        "\n",
        "print(f\"Train images: {len(train_files)}\")\n",
        "print(f\"Val images:   {len(val_files)}\")\n",
        "print(f\"Test images:  {len(test_files)}\")\n",
        "\n",
        "if len(train_files) == 0:\n",
        "    raise RuntimeError(\"No training images found. Check dataset structure.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Class distribution\n",
        "train_df = pd.DataFrame({\"label\": train_labels})\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.countplot(data=train_df, x=\"label\", order=sorted(train_df[\"label\"].unique()))\n",
        "plt.title(\"Training class distribution\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(train_df[\"label\"].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Image dimension summary (sample up to 200 images for speed)\n",
        "sample_paths = random.sample(train_files, min(200, len(train_files)))\n",
        "sizes = []\n",
        "for path in sample_paths:\n",
        "    try:\n",
        "        with Image.open(path) as img:\n",
        "            sizes.append(img.size)\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "size_df = pd.DataFrame(sizes, columns=[\"width\", \"height\"])\n",
        "print(size_df.describe())\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(data=size_df, x=\"width\", y=\"height\", alpha=0.6)\n",
        "plt.title(\"Sample image dimensions\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize a few samples\n",
        "n_samples = min(6, len(train_files))\n",
        "example_paths = random.sample(train_files, n_samples)\n",
        "fig, axes = plt.subplots(2, (n_samples + 1) // 2, figsize=(12, 6))\n",
        "axes = axes.flatten()\n",
        "for ax, path in zip(axes, example_paths):\n",
        "    with Image.open(path) as img:\n",
        "        ax.imshow(img)\n",
        "    ax.set_title(path.parent.name)\n",
        "    ax.axis(\"off\")\n",
        "for ax in axes[len(example_paths):]:\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Random training samples\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train / Validation / Test split\n",
        "- Uses existing `val` / `test` folders if present.\n",
        "- Otherwise stratifies splits from the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create stratified splits when needed\n",
        "if not test_files:\n",
        "    train_files, test_files, train_labels, test_labels = train_test_split(\n",
        "        train_files, train_labels, test_size=0.15, random_state=SEED, stratify=train_labels\n",
        "    )\n",
        "    print(f\"Created test split with {len(test_files)} images\")\n",
        "\n",
        "if not val_files:\n",
        "    train_files, val_files, train_labels, val_labels = train_test_split(\n",
        "        train_files, train_labels, test_size=0.15, random_state=SEED, stratify=train_labels\n",
        "    )\n",
        "    print(f\"Created validation split with {len(val_files)} images\")\n",
        "\n",
        "class_names = sorted(set(train_labels))\n",
        "label_to_index = {label: idx for idx, label in enumerate(class_names)}\n",
        "index_to_label = {idx: label for label, idx in label_to_index.items()}\n",
        "\n",
        "\n",
        "def encode_labels(labels):\n",
        "    return np.array([label_to_index[lbl] for lbl in labels], dtype=np.int32)\n",
        "\n",
        "train_label_ids = encode_labels(train_labels)\n",
        "val_label_ids = encode_labels(val_labels)\n",
        "test_label_ids = encode_labels(test_labels)\n",
        "\n",
        "print(f\"Classes: {class_names}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build TensorFlow datasets with augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Data augmentation applied on-the-fly\n",
        "augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.08),\n",
        "        layers.RandomZoom(0.15),\n",
        "        layers.RandomContrast(0.1),\n",
        "    ],\n",
        "    name=\"augmentation\",\n",
        ")\n",
        "\n",
        "def load_and_preprocess(path, label):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
        "    image = tf.image.resize(image, IMG_SIZE)\n",
        "    image = tf.cast(image, tf.float32)  # keep 0-255 range; EfficientNet preprocess handles scaling\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def make_dataset(paths, labels, training=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((np.array(paths, dtype=str), labels))\n",
        "    if training:\n",
        "        ds = ds.shuffle(buffer_size=len(paths), seed=SEED, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(load_and_preprocess, num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_dataset(train_files, train_label_ids, training=True)\n",
        "val_ds = make_dataset(val_files, val_label_ids, training=False)\n",
        "test_ds = make_dataset(test_files, test_label_ids, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handle class imbalance with class weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class_weights_array = compute_class_weight(\n",
        "    class_weight=\"balanced\", classes=np.arange(len(class_names)), y=train_label_ids\n",
        ")\n",
        "class_weights = {int(i): float(w) for i, w in enumerate(class_weights_array)}\n",
        "print(\"Class weights:\", class_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and train EfficientNet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "BACKBONE = \"B0\"  # switch to \"B3\" for EfficientNetB3\n",
        "INPUT_SHAPE = IMG_SIZE + (3,)\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 15\n",
        "DROPOUT_RATE = 0.35\n",
        "\n",
        "\n",
        "def get_backbone(name: str, input_shape):\n",
        "    name = name.upper()\n",
        "    if name == \"B3\":\n",
        "        return tf.keras.applications.EfficientNetB3(\n",
        "            include_top=False, input_shape=input_shape, weights=\"imagenet\"\n",
        "        )\n",
        "    return tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False, input_shape=input_shape, weights=\"imagenet\"\n",
        "    )\n",
        "\n",
        "\n",
        "backbone = get_backbone(BACKBONE, INPUT_SHAPE)\n",
        "backbone.trainable = False  # start with frozen backbone\n",
        "\n",
        "inputs = keras.Input(shape=INPUT_SHAPE)\n",
        "x = augmentation(inputs)\n",
        "x = tf.keras.applications.efficientnet.preprocess_input(x)\n",
        "x = backbone(x, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "outputs = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs, name=f\"efficientnet_{BACKBONE.lower()}_melanoma\")\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "checkpoint_path = \"best_melanoma_model.keras\"\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_path, monitor=\"val_loss\", save_best_only=True, verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\", patience=4, restore_best_weights=True, verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1\n",
        "    ),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_df[\"loss\"], label=\"train\")\n",
        "plt.plot(history_df[\"val_loss\"], label=\"val\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_df[\"accuracy\"], label=\"train\")\n",
        "plt.plot(history_df[\"val_accuracy\"], label=\"val\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "test_probs = model.predict(test_ds)\n",
        "test_pred = np.argmax(test_probs, axis=1)\n",
        "\n",
        "acc = accuracy_score(test_label_ids, test_pred)\n",
        "prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "    test_label_ids, test_pred, average=\"weighted\", zero_division=0\n",
        ")\n",
        "\n",
        "print(f\"Test Accuracy:  {acc:.4f}\")\n",
        "print(f\"Test Precision: {prec:.4f}\")\n",
        "print(f\"Test Recall:    {rec:.4f}\")\n",
        "print(f\"Test F1:        {f1:.4f}\")\n",
        "\n",
        "print(\"\n",
        "Classification report:\")\n",
        "print(classification_report(test_label_ids, test_pred, target_names=class_names, zero_division=0))\n",
        "\n",
        "cm = confusion_matrix(test_label_ids, test_pred)\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save model to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "drive.mount('/content/drive')\n",
        "MODEL_DIR = Path(\"/content/drive/MyDrive/melanoma_models\")\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "save_path = MODEL_DIR / f\"{model.name}_best.keras\"\n",
        "model.save(save_path)\n",
        "print(f\"Saved model to: {save_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}